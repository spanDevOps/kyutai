#!/bin/bash
# Kyutai STT - Simple Vast.ai Deployment (Testing Only)
# No autoscaling, no complex features - just basic live transcription

set -e

# Configuration
PROJECT_NAME="kyutai-stt"
API_PORT=${API_PORT:-8000}
BATCH_SIZE=${BATCH_SIZE:-80}
API_KEY=${API_KEY:-$(openssl rand -hex 16)}

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Banner
cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                 KYUTAI STT SIMPLE DEPLOYER                  ‚ïë
‚ïë                   Testing on Vast.ai                        ‚ïë
‚ïë                                                              ‚ïë
‚ïë  üé§ Live WebSocket transcription only                        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF

log_info "Starting simple deployment for testing..."

# Check if this is a resumed deployment after reboot
if [ -f "/opt/kyutai-stt/.deployment_state" ]; then
    log_info "üîÑ Detected previous deployment state - resuming..."
    cd /opt/kyutai-stt
    source .deployment_state
    
    # Check if services are already running (only if Docker is available)
    if command -v docker &> /dev/null && docker info &> /dev/null; then
        if docker-compose ps 2>/dev/null | grep -q "Up"; then
            log_success "Services are already running! Skipping to completion..."
            PUBLIC_IP=$(curl -s ifconfig.me 2>/dev/null || hostname -I | awk '{print $1}')
            
            echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "‚ïë                 DEPLOYMENT ALREADY COMPLETE                 ‚ïë"
            echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo "üåê WebSocket: ws://${PUBLIC_IP}:${API_PORT}/ws/live"
            echo "üåê Demo: http://${PUBLIC_IP}:${API_PORT}/demo"
            echo "üîë API Key: ${API_KEY}"
            echo "üìä Batch Size: ${BATCH_SIZE}"
            exit 0
        fi
    fi
    
    # Resume from where we left off
    log_info "Resuming deployment..."
else
    # Create deployment state file
    mkdir -p /opt/kyutai-stt
    echo "API_PORT=${API_PORT}" > /opt/kyutai-stt/.deployment_state
    echo "BATCH_SIZE=${BATCH_SIZE}" >> /opt/kyutai-stt/.deployment_state
    echo "API_KEY=${API_KEY}" >> /opt/kyutai-stt/.deployment_state
fi

# Check if we're in a container
if [ -f /.dockerenv ]; then
    log_info "üê≥ Detected container environment - switching to container-optimized deployment"
    curl -sSL https://raw.githubusercontent.com/spanDevOps/kyutai/main/container-deploy.sh | bash
    exit $?
fi

# Validate environment (for VM deployment)
if [ "$EUID" -ne 0 ]; then
    log_error "This script must be run as root. Use: sudo bash"
    exit 1
fi

# Pre-install curl for connectivity check
apt update -qq
apt install -y -qq curl openssl

# Check internet connectivity
if ! curl -s --connect-timeout 5 google.com &> /dev/null; then
    log_error "No internet connection detected. Please check your network."
    exit 1
fi
log_success "Internet connectivity verified"

# Check GPU
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -1)
    GPU_MEMORY=$(echo $GPU_INFO | cut -d',' -f2 | tr -d ' ')
    log_success "GPU detected: $GPU_INFO"
    
    if [ "$GPU_MEMORY" -lt 16000 ]; then
        BATCH_SIZE=32
        log_warning "GPU has <16GB VRAM, reducing batch size to $BATCH_SIZE"
    fi
else
    log_error "No NVIDIA GPU detected. This deployment requires a GPU."
    exit 1
fi

# Handle containerd conflicts first  
log_info "Resolving package conflicts..."
export DEBIAN_FRONTEND=noninteractive
apt-get remove -y containerd containerd.io 2>/dev/null || true
apt-get autoremove -y 2>/dev/null || true

# Install dependencies
log_info "Installing system dependencies..."

# Auto-detect if we should skip upgrade (default: yes, for faster deployment)
if [ "${FORCE_UPGRADE:-}" = "1" ]; then
    log_info "Force upgrade requested (FORCE_UPGRADE=1)"
    apt upgrade -y -qq
else
    log_info "Skipping system upgrade for faster deployment (set FORCE_UPGRADE=1 to override)"
fi
apt install -y -qq wget git docker.io

# Install Docker Compose v2
if ! command -v docker-compose &> /dev/null; then
    log_info "Installing Docker Compose v2..."
    DOCKER_COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -o '"tag_name": "[^"]*' | cut -d'"' -f4)
    curl -L "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    chmod +x /usr/local/bin/docker-compose
    log_success "Docker Compose v2 installed"
else
    log_success "Docker Compose already installed"
fi

# Install NVIDIA Container Toolkit
log_info "Installing NVIDIA Container Toolkit..."
distribution=$(. /etc/os-release; echo $ID$VERSION_ID)

# Remove existing GPG key if present to avoid prompts
rm -f /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg 2>/dev/null || true

# Install GPG key non-interactively (force overwrite)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor > /tmp/nvidia-container-toolkit-keyring.gpg
mv /tmp/nvidia-container-toolkit-keyring.gpg /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

# Remove existing repository list to avoid conflicts
rm -f /etc/apt/sources.list.d/nvidia-container-toolkit.list 2>/dev/null || true

# Add repository
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

apt update -qq && apt install -y -qq nvidia-container-toolkit
nvidia-ctk runtime configure --runtime=docker

# Ensure Docker service is running properly
log_info "Ensuring Docker service is running..."
systemctl stop docker || true
systemctl start docker
sleep 5

# Wait for Docker to be fully ready
for i in {1..30}; do
    if docker info &> /dev/null; then
        log_success "Docker is running and ready"
        break
    fi
    if [ $i -eq 30 ]; then
        log_error "Docker failed to start properly"
        systemctl status docker
        journalctl -xeu docker.service | tail -20
        exit 1
    fi
    sleep 2
done

# Test NVIDIA Docker integration
log_info "Testing NVIDIA Docker integration..."
if docker run --rm --gpus all nvidia/cuda:12.4-base-ubuntu22.04 nvidia-smi &> /dev/null; then
    log_success "NVIDIA Docker integration working"
else
    log_warning "NVIDIA Docker test failed - will try alternative image"
fi

# Setup project
PROJECT_DIR="/opt/$PROJECT_NAME"
log_info "Setting up project directory: $PROJECT_DIR"
mkdir -p "$PROJECT_DIR"
cd "$PROJECT_DIR"

# Create simple docker-compose.yml
log_info "Creating Docker configuration..."
cat > docker-compose.yml << EOF
version: '3.8'

services:
  kyutai-stt:
    build: .
    container_name: ${PROJECT_NAME}-server
    restart: unless-stopped
    
    ports:
      - "${API_PORT}:8000"
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - API_KEY=${API_KEY}
      - BATCH_SIZE=${BATCH_SIZE}
    
    volumes:
      - model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    
    runtime: nvidia

volumes:
  model_cache:
    driver: local
EOF

# Determine best CUDA image
log_info "Determining compatible CUDA image..."
CUDA_IMAGE=""
for image in "nvidia/cuda:12.4-devel-ubuntu22.04" "nvidia/cuda:12.1-devel-ubuntu22.04" "nvidia/cuda:11.8-devel-ubuntu22.04"; do
    if docker pull $image &> /dev/null; then
        CUDA_IMAGE=$image
        log_success "Using CUDA image: $CUDA_IMAGE"
        break
    fi
done

if [ -z "$CUDA_IMAGE" ]; then
    log_error "No compatible CUDA image found"
    exit 1
fi

# Create simple Dockerfile
cat > Dockerfile << EOF
FROM $CUDA_IMAGE

ENV DEBIAN_FRONTEND=noninteractive
ENV RUST_VERSION=1.75.0
ENV RUSTUP_HOME=/opt/rust
ENV CARGO_HOME=/opt/rust
ENV PATH=/opt/rust/bin:\$PATH

# Install dependencies
RUN apt-get update && apt-get install -y \\
    curl wget git build-essential pkg-config libssl-dev \\
    python3 python3-pip python3-dev cmake \\
    && rm -rf /var/lib/apt/lists/*

# Install Rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \\
    sh -s -- -y --default-toolchain \$RUST_VERSION --profile minimal
RUN chmod -R a+w /opt/rust

# Install Python dependencies
RUN pip3 install --no-cache-dir \\
    fastapi uvicorn[standard] websockets msgpack \\
    soundfile numpy

WORKDIR /app

# Copy configs (downloaded by build script)
COPY configs/ ./configs/

# Create Python files by copying them in the build script
COPY live_api_server.py .

# Create entrypoint script by copying
COPY docker-entrypoint.sh .
RUN chmod +x docker-entrypoint.sh

# Install moshi-server
RUN /opt/rust/bin/cargo install --features cuda moshi-server

# Update batch size
ARG BATCH_SIZE=80
RUN sed -i "s/batch_size = 64/batch_size = \${BATCH_SIZE}/" configs/config-stt-en_fr-hf.toml

# Create directories
RUN mkdir -p /app/logs

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

ENTRYPOINT ["./docker-entrypoint.sh"]
EOF

# Download configs from official Kyutai repository
log_info "Downloading configuration files..."
if git clone --depth 1 https://github.com/kyutai-labs/delayed-streams-modeling.git temp_kyutai; then
    cp -r temp_kyutai/configs ./
    rm -rf temp_kyutai
    log_success "Configuration files downloaded"
else
    log_error "Failed to download configuration files"
    exit 1
fi

# Check available disk space
AVAILABLE_SPACE=$(df . | awk 'NR==2 {print $4}')
REQUIRED_SPACE=15000000  # 15GB in KB
if [ "$AVAILABLE_SPACE" -lt "$REQUIRED_SPACE" ]; then
    log_error "Insufficient disk space. Required: 15GB, Available: $((AVAILABLE_SPACE/1000000))GB"
    log_error "Docker build requires significant space for Rust compilation and model cache"
    exit 1
fi
log_success "Disk space check passed: $((AVAILABLE_SPACE/1000000))GB available"

# Create live_api_server.py
log_info "Creating API server file..."
cat > live_api_server.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import websockets
import msgpack
import numpy as np
import json
import time
import logging
import os
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Kyutai STT Live API", version="2.0.0")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

MOSHI_SERVER_URL = "ws://localhost:8080/api/asr-streaming"
API_KEY = os.getenv("API_KEY", "public_token")
SAMPLE_RATE = 24000
CHUNK_SIZE = 1920

class LiveTranscriptionService:
    def __init__(self):
        self.active_connections = 0
    
    async def handle_live_transcription(self, client_websocket: WebSocket):
        self.active_connections += 1
        try:
            headers = {"kyutai-api-key": API_KEY}
            async with websockets.connect(MOSHI_SERVER_URL, additional_headers=headers) as moshi_ws:
                logger.info(f"Live session started. Active: {self.active_connections}")
                client_to_moshi = asyncio.create_task(self._forward_audio_to_moshi(client_websocket, moshi_ws))
                moshi_to_client = asyncio.create_task(self._forward_transcription_to_client(moshi_ws, client_websocket))
                done, pending = await asyncio.wait([client_to_moshi, moshi_to_client], return_when=asyncio.FIRST_COMPLETED)
                for task in pending:
                    task.cancel()
        except Exception as e:
            logger.error(f"Live transcription error: {str(e)}")
            try:
                await client_websocket.send_json({"type": "error", "message": str(e)})
            except:
                pass
        finally:
            self.active_connections -= 1
            logger.info(f"Session ended. Active: {self.active_connections}")
    
    async def _forward_audio_to_moshi(self, client_ws: WebSocket, moshi_ws):
        try:
            while True:
                message = await client_ws.receive()
                if message["type"] == "websocket.receive" and "bytes" in message:
                    audio_data = np.frombuffer(message["bytes"], dtype=np.float32)
                    if len(audio_data) == CHUNK_SIZE:
                        moshi_message = {"type": "Audio", "pcm": [float(x) for x in audio_data]}
                        msg_bytes = msgpack.packb(moshi_message, use_bin_type=True, use_single_float=True)
                        await moshi_ws.send(msg_bytes)
        except WebSocketDisconnect:
            pass
        except Exception as e:
            logger.error(f"Error forwarding audio: {e}")
    
    async def _forward_transcription_to_client(self, moshi_ws, client_ws: WebSocket):
        try:
            async for message in moshi_ws:
                try:
                    data = msgpack.unpackb(message, raw=False)
                    if data.get("type") == "Word" and "text" in data:
                        word = data["text"].strip()
                        if word:
                            await client_ws.send_json({"type": "word", "text": word, "timestamp": time.time()})
                except Exception as e:
                    logger.error(f"Error processing message: {e}")
        except Exception as e:
            logger.error(f"Error forwarding transcription: {e}")

live_service = LiveTranscriptionService()

@app.get("/")
async def root():
    return {"service": "Kyutai STT Live API", "websocket_endpoint": "/ws/live", "active_connections": live_service.active_connections}

@app.get("/health")
async def health_check():
    try:
        headers = {"kyutai-api-key": API_KEY}
        async with websockets.connect(MOSHI_SERVER_URL, additional_headers=headers) as websocket:
            await websocket.close()
        return {"status": "healthy", "active_connections": live_service.active_connections}
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")

@app.get("/metrics")
async def get_metrics():
    return {"active_connections": live_service.active_connections, "sample_rate": SAMPLE_RATE, "chunk_size": CHUNK_SIZE}

@app.websocket("/ws/live")
async def websocket_live_transcription(websocket: WebSocket):
    await websocket.accept()
    logger.info("WebSocket connected")
    try:
        await websocket.send_json({"type": "connected", "message": "Live transcription ready", "sample_rate": SAMPLE_RATE, "chunk_size": CHUNK_SIZE})
        await live_service.handle_live_transcription(websocket)
    except WebSocketDisconnect:
        logger.info("WebSocket disconnected")
    except Exception as e:
        logger.error(f"WebSocket error: {str(e)}")

@app.get("/demo", response_class=HTMLResponse)
async def demo_page():
    # Fixed JavaScript template literals for bash
    html_content = """<!DOCTYPE html><html><head><title>Kyutai STT Demo</title><style>body{font-family:Arial,sans-serif;margin:40px}.container{max-width:800px;margin:0 auto}.status{padding:10px;margin:10px 0;border-radius:5px}.connected{background-color:#d4edda;color:#155724}.disconnected{background-color:#f8d7da;color:#721c24}.transcription{background-color:#f8f9fa;padding:20px;margin:20px 0;border-radius:5px;min-height:100px}button{padding:10px 20px;margin:10px;font-size:16px}.start{background-color:#28a745;color:white;border:none;border-radius:5px}.stop{background-color:#dc3545;color:white;border:none;border-radius:5px}</style></head><body><div class="container"><h1>üé§ Kyutai STT Live Demo</h1><div id="status" class="status disconnected">Disconnected</div><button id="startBtn" class="start" onclick="startTranscription()">Start</button><button id="stopBtn" class="stop" onclick="stopTranscription()" disabled>Stop</button><h3>Live Transcription:</h3><div id="transcription" class="transcription">Click Start to begin...</div></div><script>let ws=null,audioContext=null;async function startTranscription(){try{const protocol=window.location.protocol==="https:"?"wss:":"ws:";const wsUrl=protocol+"//"+window.location.host+"/ws/live";ws=new WebSocket(wsUrl);ws.onopen=()=>{document.getElementById("status").textContent="Connected";document.getElementById("status").className="status connected";document.getElementById("transcription").innerHTML="Listening..."};ws.onmessage=event=>{const data=JSON.parse(event.data);if(data.type==="word"){document.getElementById("transcription").innerHTML+=data.text+" "}};const stream=await navigator.mediaDevices.getUserMedia({audio:{sampleRate:24000,channelCount:1}});audioContext=new AudioContext({sampleRate:24000});const source=audioContext.createMediaStreamSource(stream);const processor=audioContext.createScriptProcessor(1920,1,1);processor.onaudioprocess=event=>{if(ws&&ws.readyState===WebSocket.OPEN){const inputBuffer=event.inputBuffer.getChannelData(0);ws.send(new Float32Array(inputBuffer).buffer)}};source.connect(processor);processor.connect(audioContext.destination);document.getElementById("startBtn").disabled=true;document.getElementById("stopBtn").disabled=false}catch(error){alert("Error: "+error.message)}}function stopTranscription(){if(ws){ws.close();ws=null}if(audioContext){audioContext.close();audioContext=null}document.getElementById("status").textContent="Disconnected";document.getElementById("status").className="status disconnected";document.getElementById("startBtn").disabled=false;document.getElementById("stopBtn").disabled=true}</script></body></html>"""
    return html_content

if __name__ == "__main__":
    uvicorn.run("live_api_server:app", host="0.0.0.0", port=8000, log_level="info")
EOF

# Create docker-entrypoint.sh
log_info "Creating Docker entrypoint..."
cat > docker-entrypoint.sh << 'EOF'
#!/bin/bash
set -e
echo "üöÄ Kyutai STT Starting..."
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits 2>/dev/null || echo 'Not available')"
echo "Batch Size: ${BATCH_SIZE:-80}"

if command -v nvidia-smi &> /dev/null; then
    echo "‚è≥ Waiting for GPU..."
    for i in {1..30}; do
        if nvidia-smi &> /dev/null; then
            echo "‚úÖ GPU ready!"
            break
        fi
        sleep 2
    done
fi

echo "üéØ Starting moshi-server..."
/opt/rust/bin/moshi-server worker --config configs/config-stt-en_fr-hf.toml &

echo "‚è≥ Waiting for moshi-server..."
for i in {1..120}; do
    if ps aux | grep -q "[m]oshi-server worker"; then
        echo "‚úÖ moshi-server ready!"
        break
    fi
    if [ $i -eq 120 ]; then
        echo "‚ùå moshi-server failed to start"
        exit 1
    fi
    sleep 2
done

echo "üåê Starting API server..."
exec python3 live_api_server.py
EOF

chmod +x docker-entrypoint.sh

# Update configuration
sed -i "s/batch_size = 64/batch_size = ${BATCH_SIZE}/" configs/config-stt-en_fr-hf.toml
sed -i "s/public_token/${API_KEY}/" configs/config-stt-en_fr-hf.toml

# Build and start with error handling
log_info "Building Docker image (this may take 10-15 minutes)..."
log_info "Progress: Downloading base image and installing dependencies..."

if ! docker-compose build --build-arg BATCH_SIZE=${BATCH_SIZE} --progress=plain; then
    log_error "Docker build failed"
    log_info "Checking Docker logs for errors..."
    docker-compose logs || true
    
    # Try to clean up and retry once
    log_info "Cleaning up and retrying build..."
    docker system prune -f &> /dev/null || true
    docker-compose down &> /dev/null || true
    
    if ! docker-compose build --build-arg BATCH_SIZE=${BATCH_SIZE} --no-cache; then
        log_error "Docker build failed on retry"
        log_info "Available disk space:"
        df -h
        log_info "Docker info:"
        docker info || true
        exit 1
    fi
fi

log_success "Docker image built successfully!"

log_info "Starting services..."
if ! docker-compose up -d; then
    log_error "Failed to start services"
    docker-compose logs
    exit 1
fi

# Wait for services with better feedback
log_info "Waiting for services to start (this may take 5-10 minutes for first run)..."
log_info "Model download and container startup in progress..."

# Check if container is running first
for i in {1..60}; do
    if docker-compose ps | grep -q "Up"; then
        log_success "Container is running!"
        break
    fi
    if [ $i -eq 60 ]; then
        log_error "Container failed to start"
        docker-compose logs
        exit 1
    fi
    sleep 5
done

# Health check
log_info "Checking service health..."
for i in {1..30}; do
    if curl -sf http://localhost:${API_PORT}/health &> /dev/null; then
        log_success "Service is healthy!"
        break
    fi
    if [ $i -eq 30 ]; then
        log_error "Health check failed"
        docker-compose logs
        exit 1
    fi
    sleep 10
done

# Get public IP
PUBLIC_IP=$(curl -s ifconfig.me 2>/dev/null || hostname -I | awk '{print $1}')

# Summary
log_success "üéâ SIMPLE DEPLOYMENT COMPLETED!"
echo
echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
echo "‚ïë                    DEPLOYMENT SUMMARY                       ‚ïë"
echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
echo
log_info "üåê Live Transcription API:"
echo "   ‚Ä¢ WebSocket: ws://${PUBLIC_IP}:${API_PORT}/ws/live"
echo "   ‚Ä¢ Demo Page: http://${PUBLIC_IP}:${API_PORT}/demo"
echo "   ‚Ä¢ Health: http://${PUBLIC_IP}:${API_PORT}/health"
echo
log_info "üîë Configuration:"
echo "   ‚Ä¢ API Key: ${API_KEY}"
echo "   ‚Ä¢ Batch Size: ${BATCH_SIZE}"
echo "   ‚Ä¢ GPU Memory: ${GPU_MEMORY}MB"
echo
log_info "üõ†Ô∏è Management:"
echo "   ‚Ä¢ Logs: docker-compose logs -f"
echo "   ‚Ä¢ Restart: docker-compose restart"
echo "   ‚Ä¢ Stop: docker-compose down"
echo
log_success "‚úÖ Ready for live transcription testing!"
echo "Share the WebSocket URL with your colleagues:"
echo "ws://${PUBLIC_IP}:${API_PORT}/ws/live"

# Mark deployment as complete
echo "DEPLOYMENT_COMPLETE=true" >> /opt/kyutai-stt/.deployment_state

# Final verification
log_info "üîç Final verification..."
if curl -sf "http://localhost:${API_PORT}/health" &> /dev/null; then
    log_success "‚úÖ All systems operational!"
else
    log_warning "‚ö†Ô∏è Health check warning - service may still be starting"
    log_info "Check status with: docker-compose -f /opt/kyutai-stt/docker-compose.yml logs -f"
fi

log_info "üõ†Ô∏è Troubleshooting commands:"
echo "   ‚Ä¢ Check logs: cd /opt/kyutai-stt && docker-compose logs -f"
echo "   ‚Ä¢ Restart: cd /opt/kyutai-stt && docker-compose restart"
echo "   ‚Ä¢ Stop: cd /opt/kyutai-stt && docker-compose down"
echo "   ‚Ä¢ Rebuild: cd /opt/kyutai-stt && docker-compose build --no-cache"
echo "   ‚Ä¢ Re-run script: curl -sSL https://raw.githubusercontent.com/spanDevOps/kyutai/main/simple-deploy.sh | sudo bash"
